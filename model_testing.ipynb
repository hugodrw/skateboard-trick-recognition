{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of train descriptors: 1050128\n",
      "length of each train descriptor: 426\n"
     ]
    }
   ],
   "source": [
    "# First half, create fisher vectors for classification\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import sklearn.decomposition as decomp\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from video_representation import VideoRepresentation\n",
    "from transforms import *\n",
    "from settings import *\n",
    "from visualize import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "already_computed_descriptors=True\n",
    "\n",
    "try:\n",
    "    os.makedirs(video_descriptors_path)\n",
    "except Exception as error:\n",
    "    pass\n",
    "\n",
    "train_videos = []\n",
    "test_videos = []\n",
    "\n",
    "# COMPUTE DESCRIPTORS\n",
    "if not already_computed_descriptors:\n",
    "    for id, directory in enumerate(next(os.walk(data_dir))[1]):\n",
    "        if id == 0: # debugging, skip ollie directory\n",
    "            continue\n",
    "        directory_path = join(data_dir, directory)\n",
    "        print(f'\\n________EXTRACTING DESCRIPTORS FROM {directory_path}')\n",
    "        for filename in tqdm(os.listdir(directory_path)):\n",
    "            filepath = join(directory_path, filename)\n",
    "            # check if file already exits\n",
    "            descriptors_txt_path = os.path.join(video_descriptors_path, f'{filename.split(\".\")[0]}-descriptors.txt')\n",
    "            descriptor_found = False\n",
    "            if os.path.isfile(descriptors_txt_path):\n",
    "                print('descriptor file already exists: ', descriptors_txt_path)\n",
    "                descriptor_found = True\n",
    "            if '.avi' in filename and os.path.isfile(filepath) and not descriptor_found:\n",
    "                # task 1\n",
    "                trajectories_list = trajectories_from_video(filepath)\n",
    "                # task 2\n",
    "                # saves descriptors to disk\n",
    "                descriptors_from_trajectories(trajectories_list, filename)\n",
    "\n",
    "# TRAIN\n",
    "train_lines = []\n",
    "with open(join(data_dir, 'train.txt'), 'r') as train_f:\n",
    "    train_lines = train_f.readlines()\n",
    "for l in train_lines:\n",
    "    # Handle skateboard trick file paths\n",
    "    label = 0\n",
    "    filepath = l.replace('\\n', '')\n",
    "    if 'Kickflip' in l:\n",
    "        label = 1\n",
    "    # filepath, label = l.split() junp.avi 1\n",
    "    \n",
    "    descriptor_path = join(video_descriptors_path,\n",
    "                        f'{filepath.split(\"/\")[1].replace(\".avi\", \"-descriptors.txt\")}')\n",
    "    video_representation = VideoRepresentation(filepath, np.loadtxt(descriptor_path), label)\n",
    "    train_videos.append(video_representation)\n",
    "\n",
    "all_train_descriptors = np.concatenate([v.descriptors for v in train_videos], axis=0)\n",
    "print(f'total number of train descriptors: {all_train_descriptors.shape[0]}')\n",
    "print(f'length of each train descriptor: {all_train_descriptors.shape[1]}')\n",
    "\n",
    "# init and fit the pca\n",
    "pca = decomp.PCA(pca_num_components)\n",
    "pca = pca.fit(all_train_descriptors)\n",
    "\n",
    "# transform descriptors of each video\n",
    "for v in train_videos:\n",
    "    v.pca_descriptors = pca.transform(v.descriptors)\n",
    "\n",
    "# concatenate the pca-transformed descriptors, to not transform the whole data one extra time\n",
    "all_train_descriptors = np.concatenate([v.pca_descriptors for v in train_videos], axis=0)\n",
    "print(f'length each train descriptor after pca: {all_train_descriptors.shape[1]}')\n",
    "\n",
    "# learn GMM model\n",
    "gmm = GMM(n_components=gmm_n_components, covariance_type='diag')\n",
    "gmm.fit(all_train_descriptors)\n",
    "\n",
    "# compute fisher vectors for each train video\n",
    "for v in train_videos:\n",
    "    v.fisher_vector = fisher_from_descriptors(v.pca_descriptors, gmm)\n",
    "print('calculated Fisher vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard model\n",
    "\n",
    "# initialize and fit a linear SVM\n",
    "model = LinearSVC()\n",
    "model.fit(X=[v.fisher_vector for v in train_videos], y=[v.label for v in train_videos])\n",
    "print('fitted linear SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "print('model: ', model)\n",
    "\n",
    "test_lines = []\n",
    "with open(join(data_dir, 'test.txt'), 'r') as test_f:\n",
    "    test_lines = test_f.readlines()\n",
    "for l in test_lines:\n",
    "    # Handle skateboard trick file paths\n",
    "    label = 0\n",
    "    filepath = l.replace('\\n', '')\n",
    "    if 'Kickflip' in l:\n",
    "        label = 1\n",
    "    # filepath, label = l.split() junp.avi 1\n",
    "    descriptor_path = join(video_descriptors_path,\n",
    "                        f'{filepath.split(\"/\")[1].replace(\".avi\", \"-descriptors.txt\")}')\n",
    "    video_representation = VideoRepresentation(filepath, np.loadtxt(descriptor_path), label)\n",
    "    test_videos.append(video_representation)\n",
    "\n",
    "# reduce dimension of all test descriptors using pca fitted on train data\n",
    "for v in test_videos:\n",
    "    v.pca_descriptors = pca.transform(v.descriptors)\n",
    "print('reduced dimensions of the test data')\n",
    "\n",
    "# calculate a fisher vector for each test video based on the gmm model fit on the train data\n",
    "for v in test_videos:\n",
    "    v.fisher_vector = fisher_from_descriptors(v.pca_descriptors, gmm)\n",
    "print('calculated Fisher vectors on the test data')\n",
    "\n",
    "# predict the labels of the test videos\n",
    "accuracy = model.score(X=[v.fisher_vector for v in test_videos], y=[v.label for v in test_videos])\n",
    "print(f'accuracy: {accuracy}')\n",
    "prediction = svm.predict(X=[v.fisher_vector for v in test_videos])\n",
    "for i, v in enumerate(test_videos):\n",
    "    v.predicted_label = prediction[i]\n",
    "print('prediction by video: index, true label, predicted label, path\\n')\n",
    "for i, v in enumerate(test_videos):\n",
    "    print(f'{i}    gt: {v.label}    pred: {v.predicted_label}   {v.filepath}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # to test trajectories on a single video\n",
    "    # trajectories_from_video('data_avi_hd/Ollie/Ollie20.avi', vis_flow=False, vis_trajectories=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSc_Vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
